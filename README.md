# Week-9-Lecture
Week-8-Lecture

### 딥러닝을 위한 최적화 혁명은 어떻게 가능했을까?
* 논문명: Adam(A Method for Stochastic Optimization)
* 적응적 모멘트 추정이 만든 안정적·효율적 학습 알고리즘
  
### 논문배경
* 딥러닝 모델은 고차원 매개변수와 대규모 데이터 때문에 효율적인 확률적 최적화 기법이 필요해졌다.
* 기존의 SGD는 단순하지만 학습률 조정이 어렵고, 데이터 노이즈나 비정상적(non-stationary) 목적함수에서는 한계가 컸다.
* AdaGrad는 희소(sparse)한 그래디언트에 강하지만 학습률이 너무 빠르게 감소해 후반부 성능이 저하된다.
* RMSProp은 비정상적 목적함수에 강하지만 편향 보정이 없어 특히 β₂가 큰 경우 불안정성이 있었다.
* AdaGrad의 희소성 처리 능력과 RMSProp의 비정상적 환경 대응력을 결합해, 두 방식의 장점을 통합·보완한 최적화 알고리즘이 바로 Adam이다.

### 기존 연구의 한계점
* AdaGrad는 누적 제곱 그래디언트가 계속 커져 학습률이 지나치게 작아지는 문제가 있었다.
* RMSProp은 지수이동평균을 사용하지만 초기 편향(bias) 문제를 해결하지 못해, 특히 희소 그래디언트 상황에서 발산이나 불안정이 발생했다.
  
### 논문 목표
* Adam은 1차 모멘트(평균)와 2차 모멘트(분산)의 적응적(adaptive) 추정값을 활용해 매개변수별 학습률을 자동 조정하는 새로운 최적화 알고리즘을 제안하고,
* 이 알고리즘이 확률적·고차원·비정상적 목적함수에서도 안정적이고 효율적으로 수렴함을 이론‧실험적으로 증명하는 것이 목표다.

### 논문내용(주요 혁신 및 방법론)
* Adam은 그래디언트의 평균(mₜ)과 변화량(vₜ)을 함께 계산해, 매개변수마다 필요한 학습 속도를 스스로 조절한다.
* 초기 단계에서 값이 0으로 치우치는 문제를 막기 위해 편향 보정(bias-correction)을 적용해 m̂ₜ와 v̂ₜ를 더 정확하게 만든다.
* 이 두 값을 활용해 파라미터를 업데이트하면, 너무 크게 움직이거나 발산하는 일을 효과적으로 막을 수 있다.
* 또한 Adam은 O(√T) 수준의 수렴 보장을 이론적으로 증명해 안정적인 최적화 알고리즘임을 보여준다.
* 실험에서도 CNN, MLP, 로지스틱 회귀 등 다양한 모델에서 Adagrad, RMSProp, SGD보다 더 빠르고 안정적으로 학습이 이루어졌다.

### 기여도 및 결과
* Adam은 희소 그래디언트 처리(AdaGrad의 장점)와 비정상적 문제에서의 안정성(RMSProp의 장점)을 결합해 다양한 딥러닝 모델에서 강력한 성능을 제공하는 범용 최적화 알고리즘으로 자리잡았다.
* 실험 결과, CNN·MLP·Logistic Regression 등 다양한 환경에서 기존 방법보다 빠른 수렴과 안정적 학습을 달성했다.
* 또한 이론적으로도 강한 수렴 보장을 갖추어 이후 딥러닝 연구와 실제 산업 현장 모두에서 표준적인 최적화 알고리즘으로 널리 채택되는 기반을 마련했다.

(용어 정의 1) 적응적 모멘트 추정
* 기울기(gradient)의 평균과 변화량을 따로 기록한다.
* 최근 값일수록 더 크게 반영해 “지금 상황”을 빠르게 파악한다.
* 평균은 “방향”을, 변화량은 “얼마나 불안정한지”를 알려준다.
* 두 정보를 조합해 파라미터별로 학습 속도를 자동 조절한다.
* 그래서 튀지 않고, 너무 느리지도 않게 똑똑하게 업데이트한다.

(용어 정의 2) 경사하강법
* 경사하강법은 ‘더 낮아지는 방향을 따라 조금씩 내려가면서 최저점을 찾는 방법’입니다.
* 낮은 곳은 바로 모델의 오차(LOSS)가 작은 지점입니다.
* 즉, 경사하강법은 모델의 예측이 정답에 가까워지도록, 오차가 줄어드는 방향으로 가중치를 조금씩 조정하는 학습 방식이다.

### 참고 파일

* 논문1. Adam(A Method for Stochastic Optimization).pdf
* Adam 논문 해설.pdf
* 새넌의 엔트로피와 AI 최적화.pdf



